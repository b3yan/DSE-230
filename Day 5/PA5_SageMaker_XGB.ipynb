{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faf47a9",
   "metadata": {},
   "source": [
    "# DSE230: Programming Assignment 5 - XGBoost using SageMaker \n",
    "\n",
    "## Classification on Amazon SageMaker\n",
    "\n",
    "Perform a classification task on the given dataset.<br>\n",
    "Using the features given, you will train a XGBoost decision tree model to predict a given person's salary (the `WAGP` column) - which will be categorized into multiple bins.<br>\n",
    "\n",
    "--- \n",
    "\n",
    "#### Tasks: \n",
    "\n",
    "- Perform Exploratory Data Analysis on the given dataset\n",
    "- Save preprocessed datasets to Amazon S3\n",
    "- Use the Amazon Sagemaker platform to train an XGBoost model\n",
    "- Evaluate the model on the test set\n",
    "- Perform hyperparameter tuning on the XGBoost model\n",
    "- Submit\n",
    "  - Submit this Jupyter Notebook (`.ipynb`) to \"PA5\"\n",
    "  - Screenshot of SageMaker dashboard showing no running jobs (nothing should be in green).\n",
    "  - Make sure all the cell outputs are present in the notebook\n",
    "  - You can put both the `.ipynb` and the screenshot in a `.zip` file for submission.\n",
    "  \n",
    "#### Due date: Thursday 6/10/2021 at 11:59 PM PST\n",
    "\n",
    "---\n",
    "\n",
    "Remember: when in doubt, read the documentation first. It's always helpful to search for the class that you're trying to work with, e.g. pyspark.sql.DataFrame. \n",
    "\n",
    "Pandas API documentation: https://pandas.pydata.org/pandas-docs/stable/reference/index.html\n",
    "\n",
    "Amazon Sagemaker API documentation: https://sagemaker.readthedocs.io/en/stable/\n",
    "\n",
    "Amazon Sagemaker Tutorials: https://docs.aws.amazon.com/sagemaker/latest/dg/gs.html \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ccbd6",
   "metadata": {},
   "source": [
    "### 1. Import packages and Get Amazon IAM execution role & instance region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879d7de",
   "metadata": {},
   "source": [
    "Make sure to create an S3 bucket or re-use the ones from prior exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role- this will be necessary when defining your model\n",
    "iam_role = get_execution_role()\n",
    "\n",
    "# Set SageMaker session handle\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# set the region of the instance and get a reference to the client\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = << BUCKET NAME >>\n",
    "\n",
    "print('Using bucket ' + bucket)\n",
    "print(\"Success - the SageMaker instance is in the \" + region + \" region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61030cfc",
   "metadata": {},
   "source": [
    "### 2. Read data.\n",
    "\n",
    "NOTE - Upload the data to your S3 bucket before this step. Make sure it is in `.csv` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Read data from the S3 bucket\n",
    "file_path = << PATH TO S3 OBJECT >>\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6ad62",
   "metadata": {},
   "source": [
    "### Description of Columns\n",
    "\n",
    "There are lots of columns in the original dataset. However, we'll only use the following columns whose descriptions are given below.\n",
    "\n",
    "\n",
    "AGEP -  Age\n",
    "\n",
    "COW - Class of worker\n",
    "\n",
    "WAGP - Wages or salary income past 12 months\n",
    "\n",
    "JWMNP - Travel time to work\n",
    "\n",
    "JWTR - Means of transportation to work\n",
    "\n",
    "MAR - Marital status\n",
    "\n",
    "PERNP - Total person's earnings\n",
    "\n",
    "NWAV - Available for work\n",
    "\n",
    "NWLA - On layoff from work\n",
    "\n",
    "NWLK - Looking for work\n",
    "\n",
    "NWAB - Temporary absence from work\n",
    "\n",
    "SCHL - Educational attainment\n",
    "\n",
    "WKW - Weeks worked during past 12 months\n",
    "\n",
    "Task:\n",
    "* Select the given column names below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a22aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['AGEP', 'COW', 'WAGP', 'JWMNP', 'JWTR', 'MAR', 'PERNP', 'NWAV', \n",
    "            'NWLA', 'NWLK', 'NWAB', 'SCHL', 'WKW']\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda8e4c",
   "metadata": {},
   "source": [
    "### 3. Filtering data\n",
    "\n",
    "Find the correlation of the WAGP value with all other features.\n",
    "You can use the following technique for finding correlation between two columns:\n",
    "\n",
    "`df['col_1'].corr(df['col_2'])` gives you the correlation between col_1 and col_2.\n",
    "\n",
    "Your task is to find the correlation between WAGP and all other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0b066",
   "metadata": {},
   "source": [
    "From the results of the above cell, you should see that `PERNP` is highly correlated with `WAGP`.\n",
    "Since `PERNP` is highly correlated with `WAGP` remove that column from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb125b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['AGEP', 'COW', 'WAGP', 'JWMNP', 'JWTR', 'MAR', 'NWAV', 'NWLA', 'NWLK', 'NWAB', 'SCHL', 'WKW']\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0823e2",
   "metadata": {},
   "source": [
    "See the statistics of the target variable. Use the `.describe()` method to see the statistics of the WAGP column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df7da7",
   "metadata": {},
   "source": [
    "### 4. Outlier Removal\n",
    "\n",
    "Remove outlier rows based on values in the `WAGP` column. This will be an important step that impacts our model's predictive performance in the classification step below.\n",
    "\n",
    "Based on the statistics above, we need an **upper limit** to filter out significant outliers.\n",
    "We'll filter out all the data points for which WAGP is more than the mean + 3 standard deviations.\n",
    "\n",
    "Your tasks:\n",
    "1. Filter the dataframe using a calculated upper limit for WAGP\n",
    "\n",
    "Expected Output:\n",
    "1. Number of outlier rows removed from DataFrame\n",
    "\n",
    "Instructions:\n",
    "* Find the mean ($\\mu$) and standard deviation($\\sigma$) of the column `WAGP`\n",
    "* Set `upperLimit` to 3 standard deviations from the mean i.e. $upperLimit = \\mu + 3 \\sigma$\n",
    "* Filter the dataframe so that values in `WAGP` column are less than the upper limit i.e. `df['WAGP'] < upperLimit`\n",
    "* Print the difference in length of original dataframe and the filtered dataframe\n",
    "* For the following tasks after this step, you will use the filtered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123305f",
   "metadata": {},
   "source": [
    "### 5. Dropping NAs\n",
    "\n",
    "Drop rows with any nulls in any of the columns.<br>\n",
    "Print the resulting DataFrame's row count.\n",
    "\n",
    "**Note**: The more features you choose, the more rows with nulls you will drop. This may be desirable if you are running into memory problems<br>\n",
    "\n",
    "Your tasks:\n",
    "1. Drop rows with any nulls\n",
    "\n",
    "Expected Output: \n",
    "1. Number of rows in cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61807ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = << YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29b579",
   "metadata": {},
   "source": [
    "### 6. Discretize salary\n",
    "\n",
    "We want to convert the WAGP column, which contains continuous values, into a column with discrete labels so that we can use it as the label column for our classification problem. \n",
    "We're essentially turning a regression problem into a classification problem. Instead of predicting a person's exact salary, we're predicting the range in which that person's salary lies.\n",
    "\n",
    "Note that labels are integers and should start from 0. \n",
    "\n",
    "XGBoost expects that the Label column (WAGP_CAT) is the first column in the dataset.\n",
    "\n",
    "Your tasks:\n",
    "1. Make a new column for discretized labels with 5 bins. Recommended column name is `WAGP_CAT`\n",
    "    - XGBoost expects that the Label column (WAGP_CAT) is the first column in the dataset.\n",
    "    - Remember to put your label column as the first column in the dataframe, otherwise training won't run!\n",
    "2. Examine the label column - plot a histogram of the `WAGP_CAT` column values\n",
    "\n",
    "Expected Output: \n",
    "1. The first 5 rows of the dataframe with the discretized label column. The label column must be the first column in the dataframe. \n",
    "2. A histogram from the discretized label column\n",
    "\n",
    "* Categorize the labels into multiple bins - 5 bins in this case\n",
    "* Look up the pd.cut() function to see how the WAGP column is converted to different bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc779fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cleaned['WAGP_CAT'] = pd.cut(df_cleaned['WAGP'], bins=5, labels=[0,1,2,3,4])\n",
    "\n",
    "# Plot a histogram of the WAGP_CAT column\n",
    "<< YOUR CODE HERE >>\n",
    "\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c11ff",
   "metadata": {},
   "source": [
    "Rearranging the colums so that the WAGP_CAT column is the first column and drop WAGP (will make problem trivial otherwise). XGBoost expects labels to be in the first column. The code has been given for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_cleaned.columns.tolist()\n",
    "df_cleaned = df_cleaned[cols[-1:] + cols[:-1]].drop('WAGP', axis=1)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac0323",
   "metadata": {},
   "source": [
    "### 7. Splitting data and converting to CSV\n",
    "\n",
    " Split the dataset into train, validation, and test sets using sklearn's train_test_split. \n",
    "Look up the API definition of train_test_split to see what values you need to pass.\n",
    "First, we'll split the df_cleaned2 dataframe into two parts - `train_data` and `val_data` with an 80:20 ratio, and then\n",
    "we'll split the `train_data` into `train_data` and `test_data` in a 90:10 ratio.\n",
    "\n",
    "Use the following parameters for train_test_split:\n",
    "* `random_state = 42`\n",
    "* `shuffle = True`\n",
    "* `train_size = 0.8`, `test_size = 0.2` for the first split\n",
    "* `train_size = 0.9`, `test_size = 0.1` for the second split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723cf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = << YOUR CODE HERE >>\n",
    "\n",
    "train_data, test_data = << YOUR CODE HERE >>\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9556d",
   "metadata": {},
   "source": [
    "### Write prepared data to files.\n",
    "Refer to the demo to write the train_data, val_data, and test_data to csv files using the `.to_csv()` method\n",
    "Use `index = False` and `header = False` as the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4037dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c77c7",
   "metadata": {},
   "source": [
    "### 8. Save processed data to S3\n",
    "\n",
    "This step is needed for using XGBoost with Amazon Sagemaker. Send data to S3. SageMaker will read training data from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"data\"\n",
    "key_prefix = prefix + \"/model_data\"\n",
    "\n",
    "trainpath = sess.upload_data(\n",
    "    path='train_data.csv', bucket=bucket,\n",
    "    key_prefix=key_prefix)\n",
    "\n",
    "valpath = sess.upload_data(\n",
    "    path='val_data.csv', bucket=bucket,\n",
    "    key_prefix=key_prefix)\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path='test_data.csv', bucket=bucket,\n",
    "    key_prefix=key_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c490b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath, valpath, testpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4fc492",
   "metadata": {},
   "source": [
    "## 9. Create channels for train and validation data to feed to model\n",
    "Set up data channels for the training, validation, and test data as shown in the demo.\n",
    "You'll have to use the TrainingInput function and pass the s3_data and content_type parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = << YOUR CODE HERE >>\n",
    "s3_input_val = << YOUR CODE HERE >>\n",
    "s3_input_test = << YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d8855",
   "metadata": {},
   "source": [
    "Set model output location as shown in the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8788369",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = \"s3://{}/{}/model\".format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39751ec1",
   "metadata": {},
   "source": [
    "### 10. Create the XGBoost model\n",
    "We'll create the XGBoost model, and set its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74381195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import image_uris\n",
    "xgb_image = image_uris.retrieve(framework=\"xgboost\", region=region, version='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bcc431",
   "metadata": {},
   "source": [
    "### Create an Estimator using sagemaker.estimator.Estimator.\n",
    "You'll need to pass the xgb_image and the iam_role parameters.\n",
    "\n",
    "Use the following values for other parameters:\n",
    "* `instance_count = 1`\n",
    "* `instance_type = ml.m5.xlarge`\n",
    "* `output_path = output_location`\n",
    "* `sagemaker_session = sess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0982b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = << YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe35af",
   "metadata": {},
   "source": [
    "### 11. Set model hyperparameters\n",
    "Set the hyperparameters for the model. You'll have to use the `set_hyperparameters()` method.\n",
    "Refer to the demo for how it's done.\n",
    "\n",
    "Read the below references for more information:\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html\n",
    "https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters\n",
    "\n",
    "Use the following values for the parameters:\n",
    "* `num_class = 5`\n",
    "* `max_depth = 2`\n",
    "* `min_child_weight = 2`\n",
    "* `early_stopping_rounds=5`\n",
    "* `objective='multi:softmax'`\n",
    "* `num_round=100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a109fc",
   "metadata": {},
   "source": [
    "### 12. Train model using train and validation data channels\n",
    "Use the `.fit()` method to fit the model using the training and validation data channels. \n",
    "Execute the XGBoost training job.\n",
    "\n",
    "NOTE:  This step may take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a032e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e03dd",
   "metadata": {},
   "source": [
    "### 13. Deploying model\n",
    "Deploy the model so that it can be used for inference.\n",
    "\n",
    "Use the .deploy() method to deploy your model.\n",
    "\n",
    "Use the following values for the parameters:\n",
    "\n",
    "* `initial_instance_count = 1`\n",
    "* `instance_type = 'ml.t2.medium'`\n",
    "* `serializer = sagemaker.serializers.CSVSerializer()`\n",
    "\n",
    "NOTE:  This step may take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgb_predictor = << YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69eddee",
   "metadata": {},
   "source": [
    "### 14. Testing the model on test data\n",
    "\n",
    "* Store the values in `WAGP_CAT` column of test_data in `y_true` variable\n",
    "* Drop `WAGP_CAT` column from the test_data. Convert the resulting dataframe to an array using `.values`\n",
    "* Use the deployed model(from the previous step) to get the predictions on the test data\n",
    "* Store the value of predictions in `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0787800",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875fcf9",
   "metadata": {},
   "source": [
    "### 15. Confusion matrix and classification report\n",
    "\n",
    "Use the `confusion_matrix` and the `classification_report` methods to see how your model performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2736faa",
   "metadata": {},
   "source": [
    "### IMPORTANT: DELETE THE ENDPOINT\n",
    "\n",
    "Delete the endpoint once it has served its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ca80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6622b0",
   "metadata": {},
   "source": [
    "### 16. Hyperparameter tuning\n",
    "\n",
    "Read through the following links for more information:\n",
    "https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html\n",
    "https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/\n",
    "\n",
    "We'll use do hyperparameter tuning on two hyperparameters:\n",
    "\n",
    "1. min_child_weight\n",
    "2. max_depth\n",
    "\n",
    "We'll use a `Random` search strategy since that's more effective than searching all possible combinations of hyperparameters. The code has been given for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter\n",
    "\n",
    "# Define exploration boundaries\n",
    "hyperparameter_ranges = {\n",
    "    'min_child_weight': IntegerParameter(1, 10),\n",
    "    'max_depth': IntegerParameter(1, 10)\n",
    "}\n",
    "\n",
    "# create Optimizer\n",
    "Optimizer = HyperparameterTuner(\n",
    "    estimator=xgb,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    base_tuning_job_name='XGBoost-Tuner',\n",
    "    objective_type='Minimize',\n",
    "    objective_metric_name='validation:merror',\n",
    "    max_jobs=5,\n",
    "    max_parallel_jobs=5,\n",
    "    strategy='Random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dc0b7",
   "metadata": {},
   "source": [
    "Now that we have created the Optimizer. We need to call `.fit()` on it to start the tuning job.\n",
    "\n",
    "Refer to the demo and see how to call `fit()` and pass the appropriate data channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee049b1",
   "metadata": {},
   "source": [
    "### 17. Results of tuning job\n",
    "\n",
    "Get the tuner results in a dataframe. The code is given to you for getting the results of the tuning job in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e383e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Optimizer.analytics().dataframe()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec261c0",
   "metadata": {},
   "source": [
    "See the best hyperparameters found by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909123f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f520a",
   "metadata": {},
   "source": [
    "### 18. Deploy the tuned model.\n",
    "\n",
    "\"Use the .deploy() method to deploy the best model found by the Optimizer.\n",
    "If you call Optimizer.deploy() method, it will deploy the best model it found.\n",
    "\n",
    "Use these parameters when calling deploy:\n",
    "* `initial_instance_count=1`\n",
    "* `instance_type= 'ml.t2.medium'`\n",
    "* `serializer = sagemaker.serializers.CSVSerializer()`\n",
    "\n",
    "Refer to the demo if you are unsure of what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_predictor = << YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8044e0",
   "metadata": {},
   "source": [
    "### 19. Test the tuned model on test data\n",
    "\n",
    "* Use the deployed model(from the previous step) to get the predictions on the test data\n",
    "* Store the value of predictions in `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff27d4",
   "metadata": {},
   "source": [
    "### 20. Confusion matrix and classification report\n",
    "Use the `confusion_matrix` and the `classification_report` methods to see how your model performs on the test set.\n",
    "\n",
    "You should see that the tuned model gives you better performance in the f1-score for each (or most) of  the classses. If not, then you're probably doing something wrong.\n",
    "\n",
    "HINT - Follow instructions similar to section **14. Testing the model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ad315",
   "metadata": {},
   "source": [
    "### IMPORTANT: DELETE THE ENDPOINT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc9330",
   "metadata": {},
   "source": [
    "### 21. Screenshot of everything terminated.\n",
    "\n",
    "You need to submit a screenshot of terminated endpoints and notebook instances once you are done with the assignment. Nothing should be in green in this screenshot since all running instances are shown in green.\n",
    "\n",
    "You can take the screenshot of the Dashboard once you go to Amazon SageMaker."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
