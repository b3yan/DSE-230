{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divided-marble",
   "metadata": {},
   "source": [
    "# DSE 230: Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-doctor",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "---\n",
    "\n",
    "Remember: when in doubt, read the documentation first. It's always helpful to search for the class that you're trying to work with, e.g. pyspark.sql.DataFrame.\n",
    "\n",
    "Spark DataFrame Guide: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "PySpark API Documentation: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "Spark SQL Guide: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "Spark Streaming Guide: https://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-questionnaire",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-orleans",
   "metadata": {},
   "source": [
    "#### The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[*]'),\n",
    "                                   ('spark.app.name', 'Python Spark SQL Demo')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-apparel",
   "metadata": {},
   "source": [
    "#### Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame based on contents of a JSON file\n",
    "df = spark.read.json(\"file:/home/work/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-radius",
   "metadata": {},
   "source": [
    "##### Print Schema. Are the types correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< YOUR CODE HERE >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-tragedy",
   "metadata": {},
   "source": [
    "#### Manually specifying schema by casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"file:/home/work/people.txt\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-kansas",
   "metadata": {},
   "source": [
    "##### Specify the schema\n",
    "* Each line is assigned to \"value\" column\n",
    "* Split the value column into \"name\" and \"age\" columns\n",
    "* Assign appropriate types to columns. Assign the result to `newDf`\n",
    "* Useful functions: [DataFrame.withColumn](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withColumn), [Column.getItem](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.getItem.html), [Column.cast](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.cast.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< YOUR CODE HERE >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-wisconsin",
   "metadata": {},
   "source": [
    "##### Print the schema of `newDf`. Are the types correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< YOUR CODE HERE >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-vampire",
   "metadata": {},
   "source": [
    "#### Running SQL queries programatically\n",
    "\n",
    "The `sql` function on a `SparkSession` enables applications to run SQL queries programmatically and returns the result as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-meeting",
   "metadata": {},
   "source": [
    "##### Write an SQL query to print age and name from table `people`\n",
    "Hint - Use `spark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< YOUR CODE HERE >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-offering",
   "metadata": {},
   "source": [
    "#### Infer Schema using Reflection\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "# Lines is an RDD of rows\n",
    "lines = spark.sparkContext.textFile(\"file:/home/work/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Define a dictionary of kwargs to specify the schema\n",
    "# Name is of type str(default) and age is of type int\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the schema\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "\n",
    "# Register the DataFrame as a table.\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-arizona",
   "metadata": {},
   "source": [
    "##### Write SQL query to select the names of all teenagers( 13 <= age <= 19) from the table `people`\n",
    "Assign the result to `teenagers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< YOUR CODE HERE >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.rdd returns the content as `pyspark.RDD` of `Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-tissue",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "# Lines is an RDD of rows\n",
    "lines = spark.sparkContext.textFile(\"file:/home/work/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify the schema\n",
    "fields = [StructField(\"name\", StringType(), True),\n",
    "         StructField(\"age\", IntegerType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPeople.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
