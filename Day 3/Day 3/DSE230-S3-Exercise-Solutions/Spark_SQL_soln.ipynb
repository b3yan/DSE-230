{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opening-cruise",
   "metadata": {},
   "source": [
    "# DSE 230: Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-genome",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "---\n",
    "\n",
    "Remember: when in doubt, read the documentation first. It's always helpful to search for the class that you're trying to work with, e.g. pyspark.sql.DataFrame.\n",
    "\n",
    "Spark DataFrame Guide: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "PySpark API Documentation: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "Spark SQL Guide: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "Spark Streaming Guide: https://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-nelson",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-thursday",
   "metadata": {},
   "source": [
    "#### The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, just use SparkSession.builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAll([('spark.master', 'local[*]'),\n",
    "                                   ('spark.app.name', 'Python Spark SQL Demo')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-bargain",
   "metadata": {},
   "source": [
    "#### Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame based on contents of a JSON file\n",
    "df = spark.read.json(\"file:/home/work/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic schema inference\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-pledge",
   "metadata": {},
   "source": [
    "#### Manually specifying schema by casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from a text file\n",
    "# Each line is read into the \"value\" column\n",
    "df = spark.read.text(\"file:/home/work/people.txt\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf = df.withColumn(\"name\", split(col(\"value\"), \",\").getItem(0)) \\\n",
    "          .withColumn(\"age\", split(col(\"value\"), \",\").getItem(1).cast(\"int\"))\n",
    "newDf.select(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-actress",
   "metadata": {},
   "source": [
    "#### Running SQL queries programatically\n",
    "\n",
    "The `sql` function on a `SparkSession` enables applications to run SQL queries programmatically and returns the result as a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-assembly",
   "metadata": {},
   "source": [
    "#### Infer Schema using Reflection\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "# Lines is an RDD of rows\n",
    "lines = spark.sparkContext.textFile(\"file:/home/work/people.txt\") # Replace with spark.read.text\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Define a dictionary of kwargs to specify the schema\n",
    "# Name is of type str(default) and age is of type int\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the schema\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "\n",
    "# Register the DataFrame as a table.\n",
    "schemaPeople.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.rdd returns the content as `pyspark.RDD` of `Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-experiment",
   "metadata": {},
   "source": [
    "#### Programmatically Specifying the Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "# Lines is an RDD of rows\n",
    "lines = spark.sparkContext.textFile(\"file:/home/work/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify the schema\n",
    "fields = [StructField(\"name\", StringType(), True),\n",
    "         StructField(\"age\", IntegerType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPeople.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-filename",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
