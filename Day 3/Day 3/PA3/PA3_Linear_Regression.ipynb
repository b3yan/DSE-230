{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ethical-upset",
   "metadata": {},
   "source": [
    "# DSE 230: Programming Assignment 3 - Linear Regression\n",
    "\n",
    "#### Tasks: \n",
    "\n",
    "- Linear Regression on the Boston Housing dataset.  \n",
    "  \n",
    "- Submission on Gradescope:\n",
    "  - Submit this Jupyter Notebook as a PDF to \"PA3 Notebook\"\n",
    "  - Convert this Notebook to a .py file and submit that to \"PA3\"\n",
    "\n",
    "#### Due date: Friday 5/14/2021 at 11:59 PM PST\n",
    "\n",
    "---\n",
    "\n",
    "Remember: when in doubt, read the documentation first. It's always helpful to search for the class that you're trying to work with, e.g. pyspark.sql.DataFrame. \n",
    "\n",
    "PySpark API Documentation: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "Spark DataFrame Guide:  https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "Spark MLlib Guide: https://spark.apache.org/docs/latest/ml-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-charger",
   "metadata": {},
   "source": [
    "### Import libraries/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-prague",
   "metadata": {},
   "source": [
    "### Initialize Spark\n",
    "* Initialize Spark with 2 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-hormone",
   "metadata": {},
   "source": [
    "### Read the data from Boston_Housing.csv file\n",
    "* Print the number of rows in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-hampshire",
   "metadata": {},
   "source": [
    "### Column names in file and their description\n",
    "\n",
    "CRIM — per capita crime rate by town.\n",
    "\n",
    "ZN — proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "INDUS — proportion of non-retail business acres per town.\n",
    "\n",
    "CHAS — Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "NOX — nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "RM — average number of rooms per dwelling.\n",
    "\n",
    "AGE — proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "DIS — weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "RAD — index of accessibility to radial highways.\n",
    "\n",
    "TAX — full-value property-tax rate per $10,000.\n",
    "\n",
    "PTRATIO — pupil-teacher ratio by town.\n",
    "\n",
    "BLACK — 1000(Bk — 0.63)² where Bk is the proportion of blacks by town.\n",
    "\n",
    "LSTAT — lower status of the population (percent).\n",
    "\n",
    "MV — median value of owner-occupied homes in $1000s. This is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-mouse",
   "metadata": {},
   "source": [
    "### See one row of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-reflection",
   "metadata": {},
   "source": [
    "### Helper function for filling columns using mean or median strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "def fill_na(df, strategy):    \n",
    "    imputer = Imputer(\n",
    "        strategy=strategy,\n",
    "        inputCols=df.columns, \n",
    "        outputCols=[\"{}_imputed\".format(c) for c in df.columns]\n",
    "    )\n",
    "    \n",
    "    new_df = imputer.fit(df).transform(df)\n",
    "    \n",
    "    # Select the newly created columns with all filled values\n",
    "    new_df = new_df.select([c for c in new_df.columns if \"imputed\" in c])\n",
    "    \n",
    "    for col in new_df.columns:\n",
    "        new_df = new_df.withColumnRenamed(col, col.split(\"_imputed\")[0])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-finance",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "* Print schema to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the column names in the csv file as described above.\n",
    "col_names = ['CRIM' , 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'BLACK', 'LSTAT', 'MV']\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-sarah",
   "metadata": {},
   "source": [
    "### Drop NA's in the target variable `MV`\n",
    "* Print the number of remaining rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-gibraltar",
   "metadata": {},
   "source": [
    "### Fill the NA's for remaining columns using a mean strategy\n",
    "* Use the `fill_na` function provided above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-certificate",
   "metadata": {},
   "source": [
    "### Create feature vector using VectorAssembler\n",
    "\n",
    "* Create a vector column composed of _all_ the features\n",
    "* Don't include the label \"MV\" here since label isn't a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-production",
   "metadata": {},
   "source": [
    "### Print first 5 rows of the created dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-transfer",
   "metadata": {},
   "source": [
    "### Rename the column `MV` to `Label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-bishop",
   "metadata": {},
   "source": [
    "### Split the dataframe using the randomSplit() function \n",
    " * Train dataframe and test dataframe with a 75:25 split between them\n",
    " * Use seed=42 as one the parameters of the randomSplit() function to maintain consistency among all submissions.\n",
    " * Print the number of rows in train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = << YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-worship",
   "metadata": {},
   "source": [
    "### Use the StandardScaler to standardize your data.\n",
    "* **IMPORTANT** - Use only the training data for scaling\n",
    "* Standardize values to have zero mean and unit standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-marble",
   "metadata": {},
   "source": [
    "### Scale your training and test data with the same mean and std that you'll get from the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-bacon",
   "metadata": {},
   "source": [
    "### Use `scaler_model.mean`, `scaler_model.std` to see the mean and std for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-filling",
   "metadata": {},
   "source": [
    "### Select only the `features` and `label` columns from both train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-fitness",
   "metadata": {},
   "source": [
    "### Show the first 5 rows of the resulting train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-light",
   "metadata": {},
   "source": [
    "### Use LinearRegression for training a regression model.\n",
    "* Use maxIter = 100.\n",
    "* Use the following values for regParam and elasticNetParam and see which one works better.\n",
    "  1. regParam = 0, elasticNetParam = 0\n",
    "  2. regParam = 0.3, elasticNetParam = 0.5\n",
    "\n",
    "Look into the [API](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.regression.LinearRegression.html) specification to get more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-trinidad",
   "metadata": {},
   "source": [
    "### Print the coefficients and intercept of the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients: \" + << YOUR CODE HERE >>)\n",
    "print(\"Intercept: \" + << YOUR CODE HERE >>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-supervision",
   "metadata": {},
   "source": [
    "### Print the training results\n",
    "* Print the root mean squared error(RMSE) of the training\n",
    "* Print the coefficient of determination(r2) of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>\n",
    "\n",
    "print(\"RMSE: %f\" % << YOUR CODE HERE >>)\n",
    "print(\"r2: %f\" % << YOUR CODE HERE >>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-broad",
   "metadata": {},
   "source": [
    "### Test the model on test data\n",
    "* Print the RMSE and r2 on test data\n",
    "* Hint - Refer to [`RegressionEvaluator`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.RegressionEvaluator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "<< YOUR CODE HERE >>\n",
    "\n",
    "print(\"RMSE: %f\" % << YOUR CODE HERE >>)\n",
    "print(\"r2: %f\" % << YOUR CODE HERE >>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-annotation",
   "metadata": {},
   "source": [
    "### Plot results on test data(using matplotlib)\n",
    "\n",
    " * In the test data, you have labels, and you also have predictions for each of the test data.\n",
    " * Plot a scatter plot of the labels(in blue) and predictions(in red) on a single plot so that you can visualize how the predictions look as compared to the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-knife",
   "metadata": {},
   "source": [
    "### Add regularization to model\n",
    "* Try different values of regularization parameters `regParam` and `elasticNetParam` to see how performance changes.\n",
    "* Look into the API specification for [regParam](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html#pyspark.ml.regression.LinearRegression.regParam) and [elasticNetParam](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html#pyspark.ml.regression.LinearRegression.elasticNetParam) to get more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best elasticNetParam = \", << YOUR ANSWER HERE >>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-jacksonville",
   "metadata": {},
   "source": [
    "### Stop the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "<< YOUR CODE HERE >>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
