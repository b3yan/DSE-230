{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSE 230: Programming Assignment 4.2 - Word Count on Amazon EMR\n",
    "\n",
    "---\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "- Work with **BookReviews_1M** dataset to set up the word count exercise as you did in PA2. \n",
    "    - This is only to get you started on AWS and you won't need to report anything on this dataset except the run-time of your code. The word counts that you got from your previous assignment would (and should) be the same as the result you get from running the same experiment on AWS.\n",
    "\n",
    "\n",
    "- Find top 100 words and their counts based on word count for the **BookReviews_5M** dataset\n",
    "- Calculate average and standard deviation of execution times over 3 runs for these three settings:\n",
    "    1. BookReviews_1M - 1 master + 1 worker node \n",
    "    2. BoookReviews_5M - 1 master + 1 worker node\n",
    "    3. BookReviews_5M - 1 master + 3 worker nodes\n",
    "    \n",
    "    Note that worker nodes are also called core nodes when initializing them on AWS.\n",
    "    \n",
    "- Submission on Gradescope (4 files) under **PA4.2**\n",
    "  - Completed PySpark notebook (.ipynb) with results for 5M reviews dataset\n",
    "    - Make sure that all expected outputs are present\n",
    "  - A PDF of this Jupyter Notebook\n",
    "      - If exporting fails, download as HTML and print to PDF\n",
    "  - CSV file of first 100 rows of results for 5M reviews dataset\n",
    "  - A screenshot (.png) of your AWS EMR Cluster page showing that all clusters have been terminated.\n",
    "  - NOTE - You do NOT have to submit an exported executable (.py) unlike other PAs\n",
    "\n",
    "\n",
    "#### Due date: Friday 5/28/2021 at 11:59 PM PST\n",
    "\n",
    "---\n",
    "\n",
    "Remember: when in doubt, read the documentation first. It's always helpful to search for the class that you're trying to work with, e.g. pyspark.sql.DataFrame. \n",
    "\n",
    "PySpark API Documentation: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Upload the 1M dataset to S3\n",
    "\n",
    "To make the datasets available to the EMR cluster, we need to upload the data files to Amazon S3. Follow these steps to do so:\n",
    "\n",
    "1. In the Amazon console, open the **Services** menu on the top left and select **S3**\n",
    "2. Create a bucket if you don't have one yet. Use the default settings, but your bucket name must be unique. \n",
    "3. Create a folder in your bucket, e.g. `data`, using the default settings. (Don't upload the data file to the root of the bucket; we'll also use this bucket for later assignments, so it's good to keep everything organized.)\n",
    "4. Enter the folder and upload the **.txt** file. Do NOT upload the zip, as Spark won't know what to do with it. \n",
    "\n",
    "---\n",
    "\n",
    "You can use this dataset now. The next steps are for setting up an EMR cluster. After setting up the cluster, create a **PySpark** notebook and read the file you uploaded by copying the S3 URI, convert it to a dataframe and do anything else you want.\n",
    "\n",
    "\n",
    "\n",
    "This exercise is only to help you understand how you can create your own S3 buckets and read data from it. The actual task for you is to read data (BookReviews_5M.txt) from a different S3 bucket and work on that dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up the EMR cluster and creating a PySpark notebook\n",
    "\n",
    "We have already uploaded the 5M reviews data to the s3 bucket `s3://dse230-emr`. Follow the steps below to create an EMR cluster:\n",
    "\n",
    "1. In AWS, go to Services -> EMR.\n",
    "2. Click on 'Create cluster'.\n",
    "3. Click on 'Go to advanced options'.\n",
    "4. Select the EMR version 6.2.0, add required software packages as shown in class.\n",
    "5. Specify the instance count for master and core nodes\n",
    "6. Give your cluster a name, select an EC2 keypair that you should have created earlier. If you have not created an EC2 keypair, stop here. Go back and create a keypair first, then come back to this step.\n",
    "7. Proceed to create a cluster and wait for completion. This will take a few minutes (typically ~5-8 minutes).\n",
    "8. Go to `Notebooks` section in the sidebar of the EMR dashboard page, create a new notebook and attach it to the cluster you created earlier\n",
    "9. Open JupyterLab from this page and create a **PySpark** notebook\n",
    "8. The data is at `s3://dse230-emr/BookReviews_5M.txt`. In the following sections, use this URI for data file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start Spark Session\n",
    "\n",
    "Note that yo don't need to manually start the spark session. AWS does it for you in the background, so that the spark session is started as soon as you import pyspark. The spark session is automatically available in the global variable `spark`\n",
    "\n",
    "Remember that the kernel for running this Notebook is **PySpark** and not Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark\n",
    "\n",
    "import pyspark\n",
    "\n",
    "print (spark.version, pyspark.version.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the starting time of execution for timing this notebook\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from HDFS or S3 - For the purposes of this assignment, you should read data from HDFS\n",
    "# Although you can read directly from S3 theoretically.\n",
    "\n",
    "# Provide the HDFS file path of the 5M dataset.\n",
    "dataFileName = #YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Read data from the above file path and convert it to a dataframe. \n",
    "textDF = #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the data\n",
    "\n",
    "Your task: \n",
    "1. Examine the contents of the dataframe that you've just read from file.\n",
    "\n",
    "Expected output: \n",
    "1. Print the schema of the raw dataframe, as well as its first 25 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE to print the schema\n",
    "# YOUR CODE HERE to print the first 25 rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean the data\n",
    "\n",
    "Your task:\n",
    "1. Remove all punctuations and convert all characters to lower case.\n",
    "\n",
    "Expected output:\n",
    "1. The first 25 rows of a dataframe, with a column containing the cleaned sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell. \n",
    "\n",
    "# NOTE: Counterintuitively, column objects do NOT store any data; instead they store column expressions (transformations). \n",
    "#       The below function takes in a column object, and adds more expressions to it to make a more complex transformation. \n",
    "#       Once we have a column object representing the expressions we want, use DataFrame.select(column) to apply the expressions\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\"\"\"\n",
    "    return trim(lower(regexp_replace(column, \"[^A-Za-z0-9 ]\", \"\"))).alias(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended: take a look at the contents of a column object returned from removePunctuations. What's in there? \n",
    "# No answers or outputs required for this cell. \n",
    "print(removePunctuation(textDF.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the column expressions generated by removePunctuation() to clean the sentences\n",
    "# After that, use the show() function to print the first 25 rows of the dataframe\n",
    "# Hint: you'll need the Column object returned by removePunctuations(). \n",
    "\n",
    "# YOUR CODE HERE for printing the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Get dataframe containing unique words and their counts\n",
    "\n",
    "Your task:\n",
    "1. Split each sentence into words based on the delimiter space (' ').\n",
    "2. Put each word in each sentence row into their own rows. Put your results into a new dataframe.\n",
    "3. Print out the first 5 rows of the dataframe.\n",
    "\n",
    "\n",
    "1. First 5 rows of the output dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assemble the 'split' and 'explode' column expressions, then apply them to the sentence column\n",
    "\n",
    "# YOUR CODE HERE for printing the first 5 rows of the dataframe after the required operations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out all empty rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the dataframe by unique words, then count each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Sort the word count dataframe in a descending manner.\n",
    "\n",
    "Your task: \n",
    "1. Sort the previous dataframe by the counts column in a descending manner. Put your results into a new dataframe. \n",
    "\n",
    "Expected output:\n",
    "1. First 25 rows of the sorted word count dataframe. The first row would have the maximum count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by the 'count' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Record the execution time\n",
    "\n",
    "Your task: \n",
    "1. Print the execution time.\n",
    "\n",
    "Expected output: The execution time. No particular value is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the time since execution start - You will need this value later.\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Save the sorted word counts directly to S3 as a CSV file\n",
    "\n",
    "NOTE: Spark uses a distributed memory system, and stores working data in fragments known as \"partitions\". This is advantageous when a Spark cluster spans multiple machines, as each machine will only require part of the working data to do its own job. By default, Spark will save each of these data partitions into a individual file to avoid I/O collisions. We want only one output file, so we'll need to fuse all the data into a single partition first. \n",
    "\n",
    "Your task: \n",
    "1. Coalesce the previous dataframe to one partition. This makes sure that all our results will end up in the same CSV file. \n",
    "2. Save the 1-partition dataframe to S3 using the DataFrame.write.csv() method. Take note to store the file inside S3, at a place that you can remember. The save path should look something like `s3://<your-bucket>/<your-folder>/<your-result-file>.csv`. Change these parameters to point to your bucket and folder.\n",
    "3. Remember to save the csv file along with the header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "#### You only need to run the section 9 and section 10 once for the 5M dataset.\n",
    "#### Section 11 requires you to run multiple iterations of this Notebook, and for that you can comment out the code in section 9 so that it's easier for you to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to S3\n",
    "\n",
    "wordCountsSortedDF.coalesce(1).write.csv(\"<INSERT YOUR S3 PATH HERE>\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Download the CSV file from S3 to your local machine and create the expected CSV output file\n",
    "\n",
    "1. Navigate to the S3 folder where you stored your output\n",
    "2. Note the name of this file, it should look something like `part-00000-xx.....xx.csv`. \n",
    "3. Click on this file, it should open the file properties.\n",
    "4. Beside 'Copy S3 URI', click on 'Object actions' and then click on 'Download'.\n",
    "5. After downloading the file, you can rename it to anthing, say `results.csv`. \n",
    "6. We want you to submit a CSV containing the first 101 rows of the results file. To do this, use the command `head -n 101 results.csv > 101_rows.csv` on a terminal. You can also do so manually, since CSV files are in plain text. Remember that we want the first 101 lines which would include the header as well - so basically it is header + 100 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Execution times on different dataset and settings.\n",
    "\n",
    "You need to experiment with using different number of master and worker nodes for running this whole Jupyter Notebook. You will have to report the execution time of this Notebook as you noted in an earlier section.\n",
    "                                   \n",
    "1. Create a cluster with the required number of master and worker nodes.\n",
    "2. Then go to the Kernel tab in JupyterLab, and do 'Restart and run all cells.'\n",
    "3. You should note the time in the cell just before section 9 - this is the time that it took for all the code to run.\n",
    "4. Then, start a new cluster with a different configuration of master and worker nodes and dataset as expected. Run the Notebook again, and note the execution times.\n",
    "\n",
    "Fill in the times in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset | #Master Nodes | #Core Nodes | Runtime_1 | Runtime_2 | Runtime_3 | Mean | Std |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 1M | 1 | 1 | | | | | | \n",
    "| 5M | 1 | 1 | | | | | | \n",
    "| 5M | 1 | 3 | | | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Screenshots of terminated EMR clusters\n",
    "\n",
    "You need to attach a screenshot of your Amazon EMR 'Clusters' page which shows that all of your clusters have been terminated after you are done with your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
